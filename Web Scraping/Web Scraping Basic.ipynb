{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7335e5cc",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "- Web scraping is a general term for techniques involving automating the gathering of data from a website.\n",
    "\n",
    "- Prerequisites for web scraping:\n",
    "\n",
    "    - Basic knowledge of **HTML, CSS**.\n",
    "    \n",
    "    - **Inspecting** a webpage for the required components.\n",
    "    \n",
    "- We should keep certain things in mind before doing web scraping:\n",
    "\n",
    "    - Websites and their content are often protected by **copyrights**. So, before scraping the website, look how it's *policies, rules, guidelines, laws* etc. If required, get *permission* to scrape the website.\n",
    "    \n",
    "    - If you make too many requests to scrape a website, your IP address might be blocked for a certain amount of time. This is due to **Rate Limiting Mechanisms** in place.\n",
    "    \n",
    "    - Some websites may automatically **block** web scraping softwares.\n",
    "    \n",
    "- Before going into web scraping, set your expectations right. \n",
    "    \n",
    "    - Each website is **unique**. Therefore a web scraping script you develop will be unique to that website.\n",
    "    \n",
    "    - Often, websites change their html. In such cases, a previous web scraping script might **break** and needs to be re-written.\n",
    "    \n",
    "- Now, for basic web scraping, there are 2 fundamental libraries in python:\n",
    "\n",
    "    - `requests` library: used to make a HTTP request to the desired website and get the HTML content of the webpage.\n",
    "    \n",
    "    - `BeautifulSoup` library: used to parse and grab things from the HTML content.\n",
    "    \n",
    "- `BeautifulSoup` requires a parser to parse the HTML. A common parser is the `lxml` parser.\n",
    "\n",
    "- So, you need to install the following libraries (using the `pip install` command):\n",
    "\n",
    "    - `requests`\n",
    "    \n",
    "    - `bs4`\n",
    "    \n",
    "    - `lxml`\n",
    "    \n",
    "- There's actually a specific website designed to practice Web Scraping: [toscrape.com](https://toscrape.com/), we will use this it in the another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d580f7",
   "metadata": {},
   "source": [
    "## Importing the required libraries\n",
    "\n",
    "- First step is to import the required libraries i.e, `requests` and `bs4`.\n",
    "\n",
    "- `lxml` will be used by `bs4` in the backend, and we specify this later on in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29da34",
   "metadata": {},
   "source": [
    "## Basic Syntax\n",
    "\n",
    "- In the requests module, there is a `get()` method, which makes a HTTP GET request to a website and returns the HTML content as a special type of object.\n",
    "\n",
    "- To get a response from a website, we use the following syntax: `res = requests.get('WEBSITE_URL')`\n",
    "\n",
    "- While specifying the Website's URL, make sure you specify either `https` or `http`.\n",
    "\n",
    "- If you print the `res` object, you also see the status code of the response i.e, `200` or `404`, etc.\n",
    "\n",
    "- Now, make a request to [example.com](https://www.example.com).\n",
    "\n",
    "- To view the HTML content of a response, we can use the `text` property available on the response object. It returns a string of HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bf20c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# make a request to example.com\n",
    "response = requests.get('https://example.com/')\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b758cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .text property of the response object is the HTML document, as a string\n",
    "print(type(response.text))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32f520",
   "metadata": {},
   "source": [
    "## Parsing Response HTML\n",
    "\n",
    "- Now, we use `BeautifulSoup` library to parse the HTML string.\n",
    "\n",
    "- In the `bs4` package, there is a `BeautifulSoup` class which has the following syntax: `bs4.BeautifulSoup(HTML_String, parser)`.\n",
    "\n",
    "- The second argument of the constructor is the HTML Parser we want to use, in our case, `lxml` parser.\n",
    "\n",
    "- After parsing, we get a special `soup` object which has various functionalities we need for scraping the content off of the HTML of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749ba7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "<title>Example Domain</title>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "<style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "<div>\n",
      "<h1>Example Domain</h1>\n",
      "<p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "<p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parse the HTML using BeautifulSoup, using lxml parser as the parsing engine\n",
    "# returns a special object\n",
    "soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "print(type(soup))\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176979d",
   "metadata": {},
   "source": [
    "## Grabbing elements\n",
    "\n",
    "- On the `soup` object, we have a `select()` method, which grabs the HTML elements based some sort of **CSS Selector** and has the following syntax: `soup.select('CSS_SELECTOR')`.\n",
    "\n",
    "- It returns a `list` of all the elements which have that CSS Selector.\n",
    "\n",
    "- In the example below, we are grabbing the `title` of the webpage. We are using the **name selector**. Note that the return type is a kind of `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce695c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "[<title>Example Domain</title>]\n"
     ]
    }
   ],
   "source": [
    "# to grab the elements, use the .select() method\n",
    "print(type(soup.select('title')))\n",
    "print(soup.select('title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632eea99",
   "metadata": {},
   "source": [
    "- Now, since we want a single elements, we can use **indexing** on the ResultSet object to grab the specific element.\n",
    "\n",
    "- In the below code, we are grabbing the `title` element, and extracting the content inside the `title` tag using a `getText()` method on the `Tag` object, which is a special kind of object for each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677eb715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "<title>Example Domain</title>\n",
      "<class 'str'>\n",
      "Example Domain\n"
     ]
    }
   ],
   "source": [
    "page_title_element = soup.select('title')[0]\n",
    "print(type(page_title_element))\n",
    "print(page_title_element)\n",
    "page_title = page_title_element.getText()\n",
    "print(type(page_title))\n",
    "print(page_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0598d",
   "metadata": {},
   "source": [
    "## Example: Grabbing elements based on class name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bde68",
   "metadata": {},
   "source": [
    "- As an argument to the `.select()` method of the `soup` object, we simply pass a CSS Selector.\n",
    "\n",
    "- From [this wikipedia article](https://en.wikipedia.org/wiki/Lee_Chong_Wei), lets grab all the table of content items in the below example.\n",
    "\n",
    "> **NOTE:** Wikipedia's copyright guidelines allow us to use their content for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0775ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early life\n",
      "Career\n",
      "2002â€“2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "Doping\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "Retirement\n",
      "Personal life\n",
      "Awards\n",
      "Honours\n",
      "Achievements\n",
      "Career finals (69 titles, 34 runners-up)\n",
      "In popular culture\n",
      "See also\n",
      "References\n",
      "External links\n"
     ]
    }
   ],
   "source": [
    "# grab all the table of contents from a wikipedia article\n",
    "response = requests.get('https://en.wikipedia.org/wiki/Lee_Chong_Wei')\n",
    "\n",
    "soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "table_of_contents = soup.select('span.toctext')\n",
    "\n",
    "for item in table_of_contents:\n",
    "    print(item.getText())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a53887",
   "metadata": {},
   "source": [
    "## Example: Grabbing & Downloading Images\n",
    "\n",
    "- Typically, all images on a webpage have their own URL.\n",
    "\n",
    "- So, once we parse `img` tags and get the `src` attributes, we make another requests to the specific image URL.\n",
    "\n",
    "- When we make a request to a image, on the `response`, we have a `content` property, which is the `binary content` of the image.\n",
    "\n",
    "- So, to locally save that image, we open a file in `wb` (Write Binary) mode and write the content.\n",
    "\n",
    "- In the below example, we are grabbing and downloading 3 images from [this wikipedia article](https://en.wikipedia.org/wiki/Lin_Dan).\n",
    "\n",
    "> Before downloading & using the image, make sure you check the copyright guidelines and obtain the required permission,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8abe96e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://en.wikipedia.org/wiki/Lin_Dan')\n",
    "\n",
    "soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "image_elements = soup.select('#bodyContent img.thumbimage')\n",
    "\n",
    "print(len(image_elements))\n",
    "\n",
    "for ind, image_el in enumerate(image_elements):\n",
    "    image_url = 'https:' + image_el['src']\n",
    "    image_response = requests.get(image_url)\n",
    "    binary_content = image_response.content\n",
    "    f = open(f'image_{ind+1}.jpg', 'wb')\n",
    "    f.write(binary_content)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
